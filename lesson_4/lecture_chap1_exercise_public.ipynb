{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture_chap1_exercise_public.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["pip install tensorflow-gpu==2.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BV5lqC3q8v4N","executionInfo":{"status":"ok","timestamp":1641381368744,"user_tz":-540,"elapsed":60317,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"outputId":"2f0b5c47-9572-4147-fa03-7c06e9a463ab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu==2.2\n","  Downloading tensorflow_gpu-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n","\u001b[K     |████████████████████████████████| 516.2 MB 4.4 kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.1.2)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (3.17.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.42.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (0.12.0)\n","Collecting tensorflow-estimator<2.3.0,>=2.2.0\n","  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n","\u001b[K     |████████████████████████████████| 454 kB 53.7 MB/s \n","\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n","  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 34.8 MB/s \n","\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 39.5 MB/s \n","\u001b[?25hCollecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.13.3)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.19.5)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.4.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (0.34.2)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.6.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.2) (3.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.3.6)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.1.1)\n","Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.2.2 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n","Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-estimator-2.2.0 tensorflow-gpu-2.2.0\n"]}]},{"cell_type":"code","source":["!pip install \"wheel==0.34.2\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVMy2TCwjqY2","executionInfo":{"status":"ok","timestamp":1641381396812,"user_tz":-540,"elapsed":3524,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"outputId":"0ffb7ef6-56fe-44b1-cafa-afdbea6a50b3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wheel==0.34.2 in /usr/local/lib/python3.7/dist-packages (0.34.2)\n"]}]},{"metadata":{"id":"Ht1-DGiJAM7f"},"cell_type":"markdown","source":["# 演習 Sequence-to-Sequence (Seq2Seq) モデル"]},{"metadata":{"id":"lImUIx2SAM7h"},"cell_type":"markdown","source":["Sequence-to-Sequence (Seq2Seq) モデルは、系列を入力として系列を出力するモデルです。\n","\n","入力系列をRNNで固定長のベクトルに変換(= Encode)し、そのベクトルを用いて系列を出力(= Decode)することから、Encoder-Decoder モデルとも呼ばれます。\n","\n","RNNの代わりにLSTMやGRUでも可能です。\n","\n","機械翻訳のほか、文書要約や対話生成にも使われます。<br>\n","今回は機械翻訳を例にとって解説していきます。"]},{"metadata":{"id":"V4sqRdFNGAIN"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"AlrUn0gcNevH","outputId":"003703a3-d37e-4fa2-b758-7232c657ceb6","executionInfo":{"status":"ok","timestamp":1641381409829,"user_tz":-540,"elapsed":6529,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n","import torch\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cu80/torch-0.4.0-cp37-cp37m-linux_x86_64.whl\u001b[0m\n","\u001b[31mERROR: Could not install requirement torch==0.4.0 from http://download.pytorch.org/whl/cu80/torch-0.4.0-cp37-cp37m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu80/torch-0.4.0-cp37-cp37m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu80/torch-0.4.0-cp37-cp37m-linux_x86_64.whl\u001b[0m\n","1.10.0+cu111\n","True\n"]}]},{"metadata":{"id":"55_hatEGIB3N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24aeb5f6-dfb9-4125-dc77-0fa08ba4ad7b","executionInfo":{"status":"ok","timestamp":1641381425263,"user_tz":-540,"elapsed":7282,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["! wget https://www.dropbox.com/s/9narw5x4uizmehh/utils.py\n","! mkdir images data\n","\n","# data取得\n","! wget https://www.dropbox.com/s/o4kyc52a8we25wy/dev.en -P data/\n","! wget https://www.dropbox.com/s/kdgskm5hzg6znuc/dev.ja -P data/\n","! wget https://www.dropbox.com/s/gyyx4gohv9v65uh/test.en -P data/\n","! wget https://www.dropbox.com/s/hotxwbgoe2n013k/test.ja -P data/\n","! wget https://www.dropbox.com/s/5lsftkmb20ay9e1/train.en -P data/\n","! wget https://www.dropbox.com/s/ak53qirssci6f1j/train.ja -P data/"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-05 11:16:58--  https://www.dropbox.com/s/9narw5x4uizmehh/utils.py\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/9narw5x4uizmehh/utils.py [following]\n","--2022-01-05 11:16:58--  https://www.dropbox.com/s/raw/9narw5x4uizmehh/utils.py\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com/cd/0/inline/BdPCh05KsE6eurVqPZxp3ef_4Z800ZGQce6NDJz3cpWn7hTbeswBsGKbe4MGjteP7ULobeSvF6GmeCb3ZhyEeBjkkFugsDDe8_-69j89KfBieU_REVPvDkn4hi-PpEz6KDWYgMyKxTRC-aTqx6fJ09fz/file# [following]\n","--2022-01-05 11:16:58--  https://uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com/cd/0/inline/BdPCh05KsE6eurVqPZxp3ef_4Z800ZGQce6NDJz3cpWn7hTbeswBsGKbe4MGjteP7ULobeSvF6GmeCb3ZhyEeBjkkFugsDDe8_-69j89KfBieU_REVPvDkn4hi-PpEz6KDWYgMyKxTRC-aTqx6fJ09fz/file\n","Resolving uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com (uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n","Connecting to uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com (uc9eb5be6fddcdcd83af8b07dab0.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 949 [text/plain]\n","Saving to: ‘utils.py’\n","\n","utils.py            100%[===================>]     949  --.-KB/s    in 0s      \n","\n","2022-01-05 11:16:59 (144 MB/s) - ‘utils.py’ saved [949/949]\n","\n","--2022-01-05 11:16:59--  https://www.dropbox.com/s/o4kyc52a8we25wy/dev.en\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/o4kyc52a8we25wy/dev.en [following]\n","--2022-01-05 11:16:59--  https://www.dropbox.com/s/raw/o4kyc52a8we25wy/dev.en\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com/cd/0/inline/BdPuf_9haotKS6b-W5mUw87bVmlZh4vfUhzbT3weOocRHqDdhDH1_YLHcLCzg8y1UwTtuO_sGwL-9cQOWAXVKkCFV83k8IpLSuYUCR3z2pSgv69RPEwOqGAtt4Pvjp9QVHE_pUgzwOrXFfVkjaDPzdIS/file# [following]\n","--2022-01-05 11:16:59--  https://uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com/cd/0/inline/BdPuf_9haotKS6b-W5mUw87bVmlZh4vfUhzbT3weOocRHqDdhDH1_YLHcLCzg8y1UwTtuO_sGwL-9cQOWAXVKkCFV83k8IpLSuYUCR3z2pSgv69RPEwOqGAtt4Pvjp9QVHE_pUgzwOrXFfVkjaDPzdIS/file\n","Resolving uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com (uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n","Connecting to uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com (uc30f3702e6bb685d3ccbd958368.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17054 (17K) [text/plain]\n","Saving to: ‘data/dev.en’\n","\n","dev.en              100%[===================>]  16.65K  --.-KB/s    in 0.002s  \n","\n","2022-01-05 11:17:00 (9.04 MB/s) - ‘data/dev.en’ saved [17054/17054]\n","\n","--2022-01-05 11:17:00--  https://www.dropbox.com/s/kdgskm5hzg6znuc/dev.ja\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/kdgskm5hzg6znuc/dev.ja [following]\n","--2022-01-05 11:17:00--  https://www.dropbox.com/s/raw/kdgskm5hzg6znuc/dev.ja\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com/cd/0/inline/BdMnuS2jvtUfwzCyGW9mLj7ZrigJSVUuFhcnhFWIe90T_sukL41wpYIGo5lU10xhVgFn6juN5Um211VLf8tJdQVEuGWnojdO7TP98l4Ch1Wgoj5nGCKj_IHsfLgLGZWO3XDaZ6loOzHWFjaxk662X_f1/file# [following]\n","--2022-01-05 11:17:00--  https://uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com/cd/0/inline/BdMnuS2jvtUfwzCyGW9mLj7ZrigJSVUuFhcnhFWIe90T_sukL41wpYIGo5lU10xhVgFn6juN5Um211VLf8tJdQVEuGWnojdO7TP98l4Ch1Wgoj5nGCKj_IHsfLgLGZWO3XDaZ6loOzHWFjaxk662X_f1/file\n","Resolving uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com (uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n","Connecting to uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com (uce77a42a012c1d86edd208ef3a3.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27781 (27K) [text/plain]\n","Saving to: ‘data/dev.ja’\n","\n","dev.ja              100%[===================>]  27.13K  --.-KB/s    in 0.02s   \n","\n","2022-01-05 11:17:01 (1.39 MB/s) - ‘data/dev.ja’ saved [27781/27781]\n","\n","--2022-01-05 11:17:01--  https://www.dropbox.com/s/gyyx4gohv9v65uh/test.en\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/gyyx4gohv9v65uh/test.en [following]\n","--2022-01-05 11:17:01--  https://www.dropbox.com/s/raw/gyyx4gohv9v65uh/test.en\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com/cd/0/inline/BdOdm2EJ7sUekPNDWpUogogUfJUe6-EFIchPq-kMq9fsmMvG2HCvAN19UvnhuTYM-7MT4JQDaCLYoGEsYJuvsL3Lnb0r_RadIrJz5axCl4vTB1gpXyXc50HLKQW5xeUbssthajzOQxbNTcKZhbs0PRMI/file# [following]\n","--2022-01-05 11:17:01--  https://ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com/cd/0/inline/BdOdm2EJ7sUekPNDWpUogogUfJUe6-EFIchPq-kMq9fsmMvG2HCvAN19UvnhuTYM-7MT4JQDaCLYoGEsYJuvsL3Lnb0r_RadIrJz5axCl4vTB1gpXyXc50HLKQW5xeUbssthajzOQxbNTcKZhbs0PRMI/file\n","Resolving ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com (ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n","Connecting to ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com (ucdcfaaa9432d95f011a294b6a97.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17301 (17K) [text/plain]\n","Saving to: ‘data/test.en’\n","\n","test.en             100%[===================>]  16.90K  --.-KB/s    in 0.002s  \n","\n","2022-01-05 11:17:02 (7.11 MB/s) - ‘data/test.en’ saved [17301/17301]\n","\n","--2022-01-05 11:17:02--  https://www.dropbox.com/s/hotxwbgoe2n013k/test.ja\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/hotxwbgoe2n013k/test.ja [following]\n","--2022-01-05 11:17:02--  https://www.dropbox.com/s/raw/hotxwbgoe2n013k/test.ja\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com/cd/0/inline/BdMp1Aib7_wlLFhQIY8cJo0r2Q9ou0GyqWty-ZhQol9DKD8dEkrmVLaW3dRsVGL3MEF8ElZeYCJH2WHIcaB01CcbZtwVQ-cinSsQhgO2_2d7pCnmnoc6RtD18sFWVF0EkGrl42ocreZM55iFwFpVcYhx/file# [following]\n","--2022-01-05 11:17:02--  https://uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com/cd/0/inline/BdMp1Aib7_wlLFhQIY8cJo0r2Q9ou0GyqWty-ZhQol9DKD8dEkrmVLaW3dRsVGL3MEF8ElZeYCJH2WHIcaB01CcbZtwVQ-cinSsQhgO2_2d7pCnmnoc6RtD18sFWVF0EkGrl42ocreZM55iFwFpVcYhx/file\n","Resolving uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com (uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n","Connecting to uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com (uc9a0dd6f3618cd6037d193a43d8.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27793 (27K) [text/plain]\n","Saving to: ‘data/test.ja’\n","\n","test.ja             100%[===================>]  27.14K  --.-KB/s    in 0.02s   \n","\n","2022-01-05 11:17:03 (1.26 MB/s) - ‘data/test.ja’ saved [27793/27793]\n","\n","--2022-01-05 11:17:03--  https://www.dropbox.com/s/5lsftkmb20ay9e1/train.en\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/5lsftkmb20ay9e1/train.en [following]\n","--2022-01-05 11:17:03--  https://www.dropbox.com/s/raw/5lsftkmb20ay9e1/train.en\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc41067f404ca764e02566e63857.dl.dropboxusercontent.com/cd/0/inline/BdOeAtjKVekJGf3ektKAX548OKRwd2-PRdL44nJtL9apJ6Qp8NGtt5WvEptnkQ2ksiz2CNG52bCMjigP4zz8FloybC3miBuDOo9Kek9v7hE3AieE7p0wav4JsXUsmZP13cOd-t78YI58yhVmMmyzZOJB/file# [following]\n","--2022-01-05 11:17:03--  https://uc41067f404ca764e02566e63857.dl.dropboxusercontent.com/cd/0/inline/BdOeAtjKVekJGf3ektKAX548OKRwd2-PRdL44nJtL9apJ6Qp8NGtt5WvEptnkQ2ksiz2CNG52bCMjigP4zz8FloybC3miBuDOo9Kek9v7hE3AieE7p0wav4JsXUsmZP13cOd-t78YI58yhVmMmyzZOJB/file\n","Resolving uc41067f404ca764e02566e63857.dl.dropboxusercontent.com (uc41067f404ca764e02566e63857.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n","Connecting to uc41067f404ca764e02566e63857.dl.dropboxusercontent.com (uc41067f404ca764e02566e63857.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1701356 (1.6M) [text/plain]\n","Saving to: ‘data/train.en’\n","\n","train.en            100%[===================>]   1.62M  --.-KB/s    in 0.1s    \n","\n","2022-01-05 11:17:04 (11.0 MB/s) - ‘data/train.en’ saved [1701356/1701356]\n","\n","--2022-01-05 11:17:04--  https://www.dropbox.com/s/ak53qirssci6f1j/train.ja\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ak53qirssci6f1j/train.ja [following]\n","--2022-01-05 11:17:04--  https://www.dropbox.com/s/raw/ak53qirssci6f1j/train.ja\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com/cd/0/inline/BdMCvWF0EobQ3Az6WPMNKqvbIb8yGIITDTFGiLWbqEdxGrlLd4Yqs7ruXh-zmrZUTJ0oRyTPGOURFvGHPsSwYkkcM6lmmSlkebybXggOE1ElGiLu_XcNuG9sOdRfj7QiMQXlmmMeT5vRwGhSOaty55gr/file# [following]\n","--2022-01-05 11:17:04--  https://uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com/cd/0/inline/BdMCvWF0EobQ3Az6WPMNKqvbIb8yGIITDTFGiLWbqEdxGrlLd4Yqs7ruXh-zmrZUTJ0oRyTPGOURFvGHPsSwYkkcM6lmmSlkebybXggOE1ElGiLu_XcNuG9sOdRfj7QiMQXlmmMeT5vRwGhSOaty55gr/file\n","Resolving uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com (uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n","Connecting to uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com (uc0e4c7d5f739b78bd8348e455fc.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2784447 (2.7M) [text/plain]\n","Saving to: ‘data/train.ja’\n","\n","train.ja            100%[===================>]   2.66M  17.0MB/s    in 0.2s    \n","\n","2022-01-05 11:17:05 (17.0 MB/s) - ‘data/train.ja’ saved [2784447/2784447]\n","\n"]}]},{"metadata":{"id":"xp5QEw8CICiO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06333006-f5a8-42f1-9bde-4197b9145f97","executionInfo":{"status":"ok","timestamp":1641381430815,"user_tz":-540,"elapsed":199,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["! ls data"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["dev.en\tdev.ja\ttest.en  test.ja  train.en  train.ja\n"]}]},{"metadata":{"id":"_HauAB3uAM7i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cba9c14-f031-44f9-914e-8d3598a67829","executionInfo":{"status":"ok","timestamp":1641381435672,"user_tz":-540,"elapsed":1378,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["import random\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from nltk import bleu_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","from utils import Vocab\n","\n","# デバイスの設定\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","torch.manual_seed(1)\n","random_state = 42\n","\n","print(torch.__version__)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"metadata":{"id":"pAmQOdx0AM7o"},"cell_type":"markdown","source":["# 1.データセットの準備\n","英語-日本語の対訳コーパスである、Tanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus )を使います。<br>\n","今回はそのうちの一部分を取り出したsmall_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods ( https://github.com/odashi/small_parallel_enja )を使用します。\n","\n","train.enとtrain.jaの中身を見てみましょう。"]},{"metadata":{"id":"gVxFp2MmAM7p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"66afef56-9c39-4aa7-b2fa-34073176cc3a","executionInfo":{"status":"ok","timestamp":1641381438992,"user_tz":-540,"elapsed":205,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["! head -10 data/train.en"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["i can 't tell who will arrive first .\n","many animals have been destroyed by men .\n","i 'm in the tennis club .\n","emi looks happy .\n","please bear this fact in mind .\n","she takes care of my children .\n","we want to be international .\n","you ought not to break your promise .\n","when you cross the street , watch out for cars .\n","i have nothing to live for .\n"]}]},{"metadata":{"id":"jSgmTKl7AM7u","outputId":"dcd75b54-bb43-40f5-896e-06f17f93c829","executionInfo":{"status":"ok","timestamp":1641381442201,"user_tz":-540,"elapsed":199,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["! head -10 ./data/train.ja"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n","多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n","私 は テニス 部員 で す 。\n","エミ は 幸せ そう に 見え ま す 。\n","この 事実 を 心 に 留め て お い て 下さ い 。\n","彼女 は 私 たち の 世話 を し て くれ る 。\n","私 達 は 国際 人 に な り た い と 思 い ま す 。\n","約束 を 破 る べ き で は あ り ま せ ん 。\n","道路 を 横切 る とき は 車 に 注意 し なさ い 。\n","私 に は 生き 甲斐 が な い 。\n"]}]},{"metadata":{"id":"UbgN52WHAM7y"},"cell_type":"markdown","source":["それぞれの文章が英語-日本語で対応しているのがわかります。"]},{"metadata":{"id":"OQhfLZ5lAM7z"},"cell_type":"markdown","source":["## 1.1データの読み込みと単語の分割"]},{"metadata":{"id":"coc6DTCUAM71","executionInfo":{"status":"ok","timestamp":1641381445091,"user_tz":-540,"elapsed":202,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def load_data(file_path):\n","    # テキストファイルからデータを読み込むメソッド\n","    data = []\n","    for line in open(file_path, encoding='utf-8'):\n","        words = line.strip().split()  # スペースで単語を分割\n","        data.append(words)\n","    return data"],"execution_count":10,"outputs":[]},{"metadata":{"id":"z5UMnxsTAM74","executionInfo":{"status":"ok","timestamp":1641381447333,"user_tz":-540,"elapsed":356,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["train_X = load_data('./data/train.en')\n","train_Y = load_data('./data/train.ja')"],"execution_count":11,"outputs":[]},{"metadata":{"id":"ryYkPteoAM76","executionInfo":{"status":"ok","timestamp":1641381448507,"user_tz":-540,"elapsed":197,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["# 訓練データと検証データに分割\n","train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=random_state)"],"execution_count":12,"outputs":[]},{"metadata":{"id":"6yB4jwiCAM79"},"cell_type":"markdown","source":["この時点で入力と教師データは以下のようになっています"]},{"metadata":{"id":"0HV1SNLAAM7-","outputId":"1a18e4a7-2c0f-45ea-e153-751c756d18d9","executionInfo":{"status":"ok","timestamp":1641381449986,"user_tz":-540,"elapsed":263,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["print('train data', train_X[0])\n","print('valid data', valid_X[0])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["train data ['where', 'shall', 'we', 'eat', 'tonight', '?']\n","valid data ['you', 'may', 'extend', 'your', 'stay', 'in', 'tokyo', '.']\n"]}]},{"metadata":{"id":"NeB7llfIAM8E"},"cell_type":"markdown","source":["## 1.2単語辞書の作成\n","データセットに登場する各単語にIDを割り振る"]},{"metadata":{"id":"b-OqjDkXAM8F","executionInfo":{"status":"ok","timestamp":1641381452697,"user_tz":-540,"elapsed":2,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["# まず特殊トークンを定義しておく\n","PAD_TOKEN = '<PAD>'  # バッチ処理の際に、短い系列の末尾を埋めるために使う （Padding）\n","BOS_TOKEN = '<S>'  # 系列の始まりを表す （Beggining of sentence）\n","EOS_TOKEN = '</S>'  # 系列の終わりを表す （End of sentence）\n","UNK_TOKEN = '<UNK>'  # 語彙に存在しない単語を表す （Unknown）\n","PAD = 0\n","BOS = 1\n","EOS = 2\n","UNK = 3"],"execution_count":14,"outputs":[]},{"metadata":{"id":"T_G7dYnTAM8I","executionInfo":{"status":"ok","timestamp":1641381454753,"user_tz":-540,"elapsed":625,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["MIN_COUNT = 2  # 語彙に含める単語の最低出現回数 再提出現回数に満たない単語はUNKに置き換えられる\n","\n","# 単語をIDに変換する辞書の初期値を設定\n","word2id = {\n","    PAD_TOKEN: PAD,\n","    BOS_TOKEN: BOS,\n","    EOS_TOKEN: EOS,\n","    UNK_TOKEN: UNK,\n","    }\n","\n","# 単語辞書を作成\n","vocab_X = Vocab(word2id=word2id)\n","vocab_Y = Vocab(word2id=word2id)\n","vocab_X.build_vocab(train_X, min_count=MIN_COUNT)\n","vocab_Y.build_vocab(train_Y, min_count=MIN_COUNT)"],"execution_count":15,"outputs":[]},{"metadata":{"id":"0xDhdQ4FAM8K","outputId":"f7ba2d18-90f8-47fe-e7b3-c7ac930eb423","executionInfo":{"status":"ok","timestamp":1641381456769,"user_tz":-540,"elapsed":194,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["vocab_size_X = len(vocab_X.id2word)\n","vocab_size_Y = len(vocab_Y.id2word)\n","print('入力言語の語彙数：', vocab_size_X)\n","print('出力言語の語彙数：', vocab_size_Y)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["入力言語の語彙数： 3725\n","出力言語の語彙数： 4405\n"]}]},{"metadata":{"id":"P-K_xHBkC5TC"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"NsoRIue9AM8P"},"cell_type":"markdown","source":["# 2.テンソルへの変換"]},{"metadata":{"id":"HsCajwO2AM8Q"},"cell_type":"markdown","source":["### 2.1 IDへの変換\n","まずはモデルが文章を認識できるように、文章を単語IDのリストに変換します"]},{"metadata":{"id":"gm6qa0fNAM8R","executionInfo":{"status":"ok","timestamp":1641381463296,"user_tz":-540,"elapsed":195,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def sentence_to_ids(vocab, sentence):\n","    # 単語(str)のリストをID(int)のリストに変換する関数\n","    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n","    ids += [EOS]  # EOSを加える\n","    return ids"],"execution_count":17,"outputs":[]},{"metadata":{"id":"lk0B0VR_AM8T","executionInfo":{"status":"ok","timestamp":1641381465246,"user_tz":-540,"elapsed":861,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["train_X = [sentence_to_ids(vocab_X, sentence) for sentence in train_X]\n","train_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in train_Y]\n","valid_X = [sentence_to_ids(vocab_X, sentence) for sentence in valid_X]\n","valid_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in valid_Y]"],"execution_count":18,"outputs":[]},{"metadata":{"id":"I4pEMlCxAM8X"},"cell_type":"markdown","source":["この時点で入力と教師データは以下のようになっている"]},{"metadata":{"id":"D6OKuYgwAM8Y","outputId":"771ced0b-7826-4732-a300-e61a40df218a","executionInfo":{"status":"ok","timestamp":1641381466991,"user_tz":-540,"elapsed":299,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["print('train data', train_X[0])\n","print('valid data', valid_X[0])"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["train data [132, 321, 28, 290, 367, 12, 2]\n","valid data [8, 93, 3532, 36, 236, 13, 284, 4, 2]\n"]}]},{"metadata":{"id":"_DlWrRnQAM8f"},"cell_type":"markdown","source":["### 2.2 DataLoaderの定義\n","データセットからバッチを取得するデータローダーを定義します\n","- この際、長さの異なる複数の系列をバッチで並列に扱えるように、短い系列の末尾を特定のシンボル（`<PAD>`など）でパディングし、バッチ内の系列の長さを最長のものに合わせる\n","- (batch_size, max_length)のサイズの行列を得るが、実際にモデルを学習させるときには、バッチをまたいで各時刻ごとに進めていくので、転置して(max_length, batch_size)の形に変える<br>（batch_first=Trueのオプションを使う場合は不要）"]},{"metadata":{"id":"YtmFgYLqAM8h","executionInfo":{"status":"ok","timestamp":1641381470468,"user_tz":-540,"elapsed":209,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def pad_seq(seq, max_length):\n","    # 系列(seq)が指定の文長(max_length)になるように末尾をパディングする\n","    res = seq + [PAD for i in range(max_length - len(seq))]\n","    return res    \n","\n","\n","class DataLoader(object):\n","\n","    def __init__(self, X, Y, batch_size, shuffle=False):\n","        \"\"\"\n","        :param X: list, 入力言語の文章（単語IDのリスト）のリスト\n","        :param Y: list, 出力言語の文章（単語IDのリスト）のリスト\n","        :param batch_size: int, バッチサイズ\n","        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n","        \"\"\"\n","        self.data = list(zip(X, Y))\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.start_index = 0\n","        \n","        self.reset()\n","    \n","    def reset(self):\n","        if self.shuffle:  # サンプルの順番をシャッフルする\n","            self.data = shuffle(self.data, random_state=random_state)\n","        self.start_index = 0  # ポインタの位置を初期化する\n","    \n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        # ポインタが最後まで到達したら初期化する\n","        if self.start_index >= len(self.data):\n","            self.reset()\n","            raise StopIteration()\n","\n","        # バッチを取得\n","        seqs_X, seqs_Y = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n","        # 入力系列seqs_Xの文章の長さ順（降順）に系列ペアをソートする\n","        seq_pairs = sorted(zip(seqs_X, seqs_Y), key=lambda p: len(p[0]), reverse=True)\n","        seqs_X, seqs_Y = zip(*seq_pairs)\n","        # 短い系列の末尾をパディングする\n","        lengths_X = [len(s) for s in seqs_X]  # 後述のEncoderのpack_padded_sequenceでも用いる\n","        lengths_Y = [len(s) for s in seqs_Y]\n","        max_length_X = max(lengths_X)\n","        max_length_Y = max(lengths_Y)\n","        padded_X = [pad_seq(s, max_length_X) for s in seqs_X]\n","        padded_Y = [pad_seq(s, max_length_Y) for s in seqs_Y]\n","        # tensorに変換し、転置する\n","        batch_X = torch.tensor(padded_X, dtype=torch.long, device=device).transpose(0, 1)\n","        batch_Y = torch.tensor(padded_Y, dtype=torch.long, device=device).transpose(0, 1)\n","\n","        # ポインタを更新する\n","        self.start_index += self.batch_size\n","\n","        return batch_X, batch_Y, lengths_X"],"execution_count":20,"outputs":[]},{"metadata":{"id":"-37frCXrAM8k"},"cell_type":"markdown","source":["# 3.モデルの構築\n","EncoderとDecoderのRNNを定義します。"]},{"metadata":{"id":"1X3oRjArAM8l"},"cell_type":"markdown","source":["### 導入：PackedSequence"]},{"metadata":{"id":"lRmj-EdbAM8m"},"cell_type":"markdown","source":["PyTorchのRNNでは、可変長の系列のバッチを効率よく計算できるように系列を表現する`PackedSequence`というクラスを用いることができます。\n","\n","入力バッチのテンソルをこの`PackedSequence`のインスタンスに変換してからRNNに入力することで、パディング部分の計算を省略することができるため、効率的な計算が可能になります。\n","\n","`PackedSequence`を作成するには、まず、系列長の異なるバッチに対してパディングを行なってください。\n","\n","ここで、パディングを行う前に各サンプルの系列長(`lengths`)を保存しておきます。"]},{"metadata":{"id":"yWAV3W89AM8n","outputId":"e5d8c5e4-70e8-432f-a55f-86bfc4470c17","executionInfo":{"status":"ok","timestamp":1641381477176,"user_tz":-540,"elapsed":323,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# 系列長がそれぞれ4,3,2の3つのサンプルからなるバッチを作成\n","batch = [[1,2,3,4], [5,6,7], [8,9]]\n","lengths = [len(sample) for sample in batch]\n","print('各サンプルの系列長:', lengths)\n","print()\n","\n","# 最大系列長に合うように各サンプルをpadding\n","_max_length = max(lengths)\n","padded = torch.tensor([pad_seq(sample, _max_length) for sample in batch])\n","print('paddingされたテンソル:\\n', padded)\n","padded = padded.transpose(0,1) # (max_length, batch_size)に転置\n","print('padding & 転置されたテンソル:\\n', padded)\n","print('padding & 転置されたテンソルのサイズ:\\n', padded.size())\n","print()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["各サンプルの系列長: [4, 3, 2]\n","\n","paddingされたテンソル:\n"," tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 0],\n","        [8, 9, 0, 0]])\n","padding & 転置されたテンソル:\n"," tensor([[1, 5, 8],\n","        [2, 6, 9],\n","        [3, 7, 0],\n","        [4, 0, 0]])\n","padding & 転置されたテンソルのサイズ:\n"," torch.Size([4, 3])\n","\n"]}]},{"metadata":{"id":"GTAq1q8mAM8q"},"cell_type":"markdown","source":["次に、パディングを行ったテンソル(`padded`)と各サンプルの元々の系列長(`lengths`)を`torch.nn.utils.rnn.pack_padded_sequence`という関数に与えると、\n","`data`と`batch_sizes`という要素を持った`PackedSequence`のインスタンス(`packed`)が作成できます。\n","- `data`: テンソルの`PAD`以外の値のみを保有するベクトル\n","- `batch_sizes`: 各時刻で計算が必要な(=`PAD`に到達していない)バッチの数を表すベクトル"]},{"metadata":{"id":"FtRm7uqIAM8s","outputId":"9454ab19-86af-44bf-a25d-12c1c45a2a42","executionInfo":{"status":"ok","timestamp":1641381481055,"user_tz":-540,"elapsed":273,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# PackedSequenceに変換（テンソルをRNNに入力する前に適用する）\n","packed = pack_padded_sequence(padded, lengths=lengths) # 各サンプルの系列長も与える\n","print('PackedSequenceのインスタンス:\\n', packed) # テンソルのPAD以外の値(data)と各時刻で計算が必要な(=PADに到達していない)バッチの数(batch_sizes)を有するインスタンス\n","print()"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["PackedSequenceのインスタンス:\n"," PackedSequence(data=tensor([1, 5, 8, 2, 6, 9, 3, 7, 4]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=None, unsorted_indices=None)\n","\n"]}]},{"metadata":{"id":"iYKaiMZDAM8w"},"cell_type":"markdown","source":["こうして得られた`PackedSequence`のインスタンスをRNNに入力します。（ここでは省略）\n","\n","RNNから出力されたテンソルは`PackedSeauence`のインスタンスのままなので、後段の計算につなぐために`torch.nn.utils.rnn.pad_packed_sequence`の関数によって通常のテンソルに戻します。"]},{"metadata":{"id":"F7BBaiVzAM8x","outputId":"1f98c024-7b9d-4fb3-a1d4-27fecd266706","executionInfo":{"status":"ok","timestamp":1641381483855,"user_tz":-540,"elapsed":200,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# PackedSequenceのインスタンスをRNNに入力する（ここでは省略）\n","output = packed\n","\n","# テンソルに戻す(RNNの出力に対して適用する)\n","output, _length = pad_packed_sequence(output)  # PADを含む元のテンソルと各サンプルの系列長を返す\n","print('PADを含む元のテンソル:\\n', output)\n","print('各サンプルの系列長:', _length)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["PADを含む元のテンソル:\n"," tensor([[1, 5, 8],\n","        [2, 6, 9],\n","        [3, 7, 0],\n","        [4, 0, 0]])\n","各サンプルの系列長: tensor([4, 3, 2])\n"]}]},{"metadata":{"id":"wzT0I8w9AM81"},"cell_type":"markdown","source":["### Encoder\n","今回はEncoder側でバッチを処理する際に、`pack_padded_sequence`関数によってtensorを`PackedSequence`に変換し、処理を終えた後に`pad_packed_sequence`関数によってtensorに戻すという処理を行います。"]},{"metadata":{"id":"NdY2WGwMAM82","executionInfo":{"status":"ok","timestamp":1641381487950,"user_tz":-540,"elapsed":265,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        \"\"\"\n","        :param input_size: int, 入力言語の語彙数\n","        :param hidden_size: int, 隠れ層のユニット数\n","        \"\"\"\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, seqs, input_lengths, hidden=None):\n","        \"\"\"\n","        :param seqs: tensor, 入力のバッチ, size=(max_length, batch_size)\n","        :param input_lengths: 入力のバッチの各サンプルの文長\n","        :param hidden: tensor, 隠れ状態の初期値, Noneの場合は0で初期化される\n","        :return output: tensor, Encoderの出力, size=(max_length, batch_size, hidden_size)\n","        :return hidden: tensor, Encoderの隠れ状態, size=(1, batch_size, hidden_size)\n","        \"\"\"\n","        emb = self.embedding(seqs) # seqsはパディング済み\n","        packed = pack_padded_sequence(emb, input_lengths) # PackedSequenceオブジェクトに変換\n","        output, hidden = self.gru(packed, hidden)\n","        output, _ = pad_packed_sequence(output)\n","        return output, hidden"],"execution_count":24,"outputs":[]},{"metadata":{"id":"eBw_ZiwDAM85"},"cell_type":"markdown","source":["### Decoder\n","今回はDecoder側ではパディング等行わないので、通常のtensorのままRNNに入力して問題ありません。"]},{"metadata":{"id":"UjKk_-9_AM86","executionInfo":{"status":"ok","timestamp":1641381491535,"user_tz":-540,"elapsed":201,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        \"\"\"\n","        :param hidden_size: int, 隠れ層のユニット数\n","        :param output_size: int, 出力言語の語彙数\n","        :param dropout: float, ドロップアウト率\n","        \"\"\"\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, seqs, hidden):\n","        \"\"\"\n","        :param seqs: tensor, 入力のバッチ, size=(1, batch_size)\n","        :param hidden: tensor, 隠れ状態の初期値, Noneの場合は0で初期化される\n","        :return output: tensor, Decoderの出力, size=(1, batch_size, output_size)\n","        :return hidden: tensor, Decoderの隠れ状態, size=(1, batch_size, hidden_size)\n","        \"\"\"\n","        emb = self.embedding(seqs)\n","        output, hidden = self.gru(emb, hidden)\n","        output = self.out(output)\n","        return output, hidden"],"execution_count":25,"outputs":[]},{"metadata":{"id":"Tf64KCf2AM88"},"cell_type":"markdown","source":["## EncoderDecoder\n","上で定義したEncoderとDecoderを用いた、一連の処理をまとめるEncoderDecoderのクラスを定義します。\n","\n","ここで、Decoder側の処理で注意する点があります。\n","\n","RNNでは、時刻$t$の出力を時刻$t+1$の入力とすることができるが、この方法でDecoderを学習させると連鎖的に誤差が大きくなっていき、学習が不安定になったり収束が遅くなったりする問題が発生します。\n","\n","\n","この問題への対策として**Teacher Forcing**というテクニックがあります。\n","これは、訓練時にはDecoder側の入力に、ターゲット系列（参照訳）をそのまま使うというものです。\n","これにより学習が安定し、収束が早くなるというメリットがありますが、逆に評価時は前の時刻にDecoderが生成したものが使われるため、学習時と分布が異なってしまうというデメリットもあります。\n","\n","\n","Teacher Forcingの拡張として、ターゲット系列を入力とするか生成された結果を入力とするかを確率的にサンプリングする**Scheduled Sampling**という手法があります。\n","\n","ここではScheduled Samplingを採用し、一定の確率に基づいてターゲット系列を入力とするか生成された結果を入力とするかを切り替えられるようにクラスを定義しておきます。"]},{"metadata":{"id":"OB9Nlcd9AM89","executionInfo":{"status":"ok","timestamp":1641381494820,"user_tz":-540,"elapsed":199,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["class EncoderDecoder(nn.Module):\n","    \"\"\"EncoderとDecoderの処理をまとめる\"\"\"\n","    def __init__(self, input_size, output_size, hidden_size):\n","        \"\"\"\n","        :param input_size: int, 入力言語の語彙数\n","        :param output_size: int, 出力言語の語彙数\n","        :param hidden_size: int, 隠れ層のユニット数\n","        \"\"\"\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = Encoder(input_size, hidden_size)\n","        self.decoder = Decoder(hidden_size, output_size)\n","\n","    def forward(self, batch_X, lengths_X, max_length, batch_Y=None, use_teacher_forcing=False):\n","        \"\"\"\n","        :param batch_X: tensor, 入力系列のバッチ, size=(max_length, batch_size)\n","        :param lengths_X: list, 入力系列のバッチ内の各サンプルの文長\n","        :param max_length: int, Decoderの最大文長\n","        :param batch_Y: tensor, Decoderで用いるターゲット系列\n","        :param use_teacher_forcing: Decoderでターゲット系列を入力とするフラグ\n","        :return decoder_outputs: tensor, Decoderの出力, \n","            size=(max_length, batch_size, self.decoder.output_size)\n","        \"\"\"\n","        # encoderに系列を入力（複数時刻をまとめて処理）\n","        _, encoder_hidden = self.encoder(batch_X, lengths_X)\n","        \n","        _batch_size = batch_X.size(1)\n","\n","        # decoderの入力と隠れ層の初期状態を定義\n","        decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long, device=device) # 最初の入力にはBOSを使用する\n","        decoder_input = decoder_input.unsqueeze(0)  # (1, batch_size)\n","        decoder_hidden = encoder_hidden  # Encoderの最終隠れ状態を取得\n","\n","        # decoderの出力のホルダーを定義\n","        decoder_outputs = torch.zeros(max_length, _batch_size, self.decoder.output_size, device=device) # max_length分の固定長\n","\n","        # 各時刻ごとに処理\n","        for t in range(max_length):\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            decoder_outputs[t] = decoder_output\n","            # 次の時刻のdecoderの入力を決定\n","            if use_teacher_forcing and batch_Y is not None:  # teacher forceの場合、ターゲット系列を用いる\n","                decoder_input = batch_Y[t].unsqueeze(0)\n","            else:  # teacher forceでない場合、自身の出力を用いる\n","                decoder_input = decoder_output.max(-1)[1]\n","                \n","        return decoder_outputs"],"execution_count":26,"outputs":[]},{"metadata":{"id":"qBTFmbwLAM9A"},"cell_type":"markdown","source":["# 4.訓練\n","### 4.1 損失関数の定義\n","基本的にはクロスエントロピーを損失関数として用いますが、パディングを行うと短い系列の末尾には`<PAD>`トークンが入るため、この部分の損失を計算しないように、マスクをかけます。"]},{"metadata":{"id":"Kt-r-nxJAM9B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641381498360,"user_tz":-540,"elapsed":300,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"outputId":"8198d968-3371-4fae-dcf4-639312f8b4d2"},"cell_type":"code","source":["mce = nn.CrossEntropyLoss(size_average=False, ignore_index=PAD) # PADを無視する\n","def masked_cross_entropy(logits, target):\n","    logits_flat = logits.view(-1, logits.size(-1)) # (max_seq_len * batch_size, output_size)\n","    target_flat = target.view(-1) # (max_seq_len * batch_size, 1)\n","    return mce(logits_flat, target_flat)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"metadata":{"id":"GgQZl0GvAM9E"},"cell_type":"markdown","source":["### 4.2学習"]},{"metadata":{"id":"qurGD8IsAM9F","executionInfo":{"status":"ok","timestamp":1641381501284,"user_tz":-540,"elapsed":204,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["# ハイパーパラメータの設定\n","num_epochs = 10\n","batch_size = 64\n","lr = 1e-3  # 学習率\n","teacher_forcing_rate = 0.2  # Teacher Forcingを行う確率\n","ckpt_path = 'model.pth'  # 学習済みのモデルを保存するパス\n","\n","model_args = {\n","    'input_size': vocab_size_X,\n","    'output_size': vocab_size_Y,\n","    'hidden_size': 256,\n","}"],"execution_count":28,"outputs":[]},{"metadata":{"id":"4xNxWJGPAM9I","executionInfo":{"status":"ok","timestamp":1641381511628,"user_tz":-540,"elapsed":8649,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["# データローダを定義\n","train_dataloader = DataLoader(train_X, train_Y, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_X, valid_Y, batch_size=batch_size, shuffle=False)\n","\n","# モデルとOptimizerを定義\n","model = EncoderDecoder(**model_args).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)"],"execution_count":29,"outputs":[]},{"metadata":{"id":"_67wGmARAM9L"},"cell_type":"markdown","source":["実際に損失関数を計算する関数を定義します。"]},{"metadata":{"id":"Adggq9xOAM9L","executionInfo":{"status":"ok","timestamp":1641381513917,"user_tz":-540,"elapsed":213,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def compute_loss(batch_X, batch_Y, lengths_X, model, optimizer=None, is_train=True):\n","    # 損失を計算する関数\n","    model.train(is_train)  # train/evalモードの切替え\n","    \n","    # 一定確率でTeacher Forcingを行う\n","    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n","    max_length = batch_Y.size(0)\n","    # 推論\n","    pred_Y = model(batch_X, lengths_X, max_length, batch_Y, use_teacher_forcing)\n","    \n","    # 損失関数を計算\n","    loss = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n","    \n","    if is_train:  # 訓練時はパラメータを更新\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    batch_Y = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()\n","    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n","\n","    return loss.item(), batch_Y, pred"],"execution_count":30,"outputs":[]},{"metadata":{"id":"QHJXFnLpAM9O"},"cell_type":"markdown","source":["ここで、Loss以外に、学習の進捗を確認するためにモデルの性能を評価する指標として、BLEUを計算します。\n","\n","BLEUは機械翻訳の分野において最も一般的な自動評価基準の一つで、予め用意した複数の参照訳と、機械翻訳モデルが出力した訳のn-gramのマッチ率に基づく指標です。\n","\n","NLTK (Natural Language Tool Kit) という自然言語処理で用いられるライブラリを用いて簡単に計算することができます。"]},{"metadata":{"id":"ImK-xzAWAM9P","executionInfo":{"status":"ok","timestamp":1641381516323,"user_tz":-540,"elapsed":207,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def calc_bleu(refs, hyps):\n","    \"\"\"\n","    BLEUスコアを計算する関数\n","    :param refs: list, 参照訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n","    :param hyps: list, モデルの生成した訳。単語のリストのリスト (例： ['I', 'have', 'a', 'pen'])\n","    :return: float, BLEUスコア(0~100)\n","    \"\"\"\n","    refs = [[ref[:ref.index(EOS)]] for ref in refs] # EOSは評価しないで良いので切り捨てる, refsのほうは複数なのでlistが一個多くかかっている\n","    hyps = [hyp[:hyp.index(EOS)] if EOS in hyp else hyp for hyp in hyps]\n","    return 100 * bleu_score.corpus_bleu(refs, hyps)"],"execution_count":31,"outputs":[]},{"metadata":{"id":"inYRxu8aAM9T"},"cell_type":"markdown","source":["それではモデルの訓練を行います。"]},{"metadata":{"id":"bz-Dx5p6AM9T","outputId":"a80d5f8f-15c8-423a-abcb-367257aa6ccd","executionInfo":{"status":"ok","timestamp":1641381969273,"user_tz":-540,"elapsed":450565,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# 訓練\n","best_valid_bleu = 0.\n","\n","for epoch in range(1, num_epochs+1):\n","    train_loss = 0.\n","    train_refs = []\n","    train_hyps = []\n","    valid_loss = 0.\n","    valid_refs = []\n","    valid_hyps = []\n","    # train\n","    for batch in train_dataloader:\n","        batch_X, batch_Y, lengths_X = batch\n","        loss, gold, pred = compute_loss(\n","            batch_X, batch_Y, lengths_X, model, optimizer, \n","            is_train=True\n","            )\n","        train_loss += loss\n","        train_refs += gold\n","        train_hyps += pred\n","    # valid\n","    for batch in valid_dataloader:\n","        batch_X, batch_Y, lengths_X = batch\n","        loss, gold, pred = compute_loss(\n","            batch_X, batch_Y, lengths_X, model, \n","            is_train=False\n","            )\n","        valid_loss += loss\n","        valid_refs += gold\n","        valid_hyps += pred\n","    # 損失をサンプル数で割って正規化\n","    train_loss = np.sum(train_loss) / len(train_dataloader.data)\n","    valid_loss = np.sum(valid_loss) / len(valid_dataloader.data)\n","    # BLEUを計算\n","    train_bleu = calc_bleu(train_refs, train_hyps)\n","    valid_bleu = calc_bleu(valid_refs, valid_hyps)\n","\n","    # validationデータでBLEUが改善した場合にはモデルを保存\n","    if valid_bleu > best_valid_bleu:\n","        ckpt = model.state_dict()\n","        torch.save(ckpt, ckpt_path)\n","        best_valid_bleu = valid_bleu\n","\n","    print('Epoch {}: train_loss: {:5.2f}  train_bleu: {:2.2f}  valid_loss: {:5.2f}  valid_bleu: {:2.2f}'.format(\n","            epoch, train_loss, train_bleu, valid_loss, valid_bleu))\n","        \n","    print('-'*80)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: train_loss: 52.02  train_bleu: 3.64  valid_loss: 48.96  valid_bleu: 5.03\n","--------------------------------------------------------------------------------\n","Epoch 2: train_loss: 43.99  train_bleu: 8.18  valid_loss: 44.94  valid_bleu: 7.61\n","--------------------------------------------------------------------------------\n","Epoch 3: train_loss: 40.31  train_bleu: 11.22  valid_loss: 42.56  valid_bleu: 11.20\n","--------------------------------------------------------------------------------\n","Epoch 4: train_loss: 37.44  train_bleu: 14.19  valid_loss: 41.61  valid_bleu: 14.39\n","--------------------------------------------------------------------------------\n","Epoch 5: train_loss: 35.09  train_bleu: 16.89  valid_loss: 40.31  valid_bleu: 13.23\n","--------------------------------------------------------------------------------\n","Epoch 6: train_loss: 33.47  train_bleu: 18.68  valid_loss: 39.80  valid_bleu: 13.01\n","--------------------------------------------------------------------------------\n","Epoch 7: train_loss: 31.83  train_bleu: 20.76  valid_loss: 40.14  valid_bleu: 16.51\n","--------------------------------------------------------------------------------\n","Epoch 8: train_loss: 30.14  train_bleu: 23.22  valid_loss: 40.18  valid_bleu: 16.30\n","--------------------------------------------------------------------------------\n","Epoch 9: train_loss: 29.43  train_bleu: 24.33  valid_loss: 40.46  valid_bleu: 17.34\n","--------------------------------------------------------------------------------\n","Epoch 10: train_loss: 28.29  train_bleu: 26.05  valid_loss: 40.82  valid_bleu: 17.58\n","--------------------------------------------------------------------------------\n"]}]},{"metadata":{"id":"Y3tlT8z9SCoF"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"BdgBkAlkAM9V"},"cell_type":"markdown","source":["# 5.評価"]},{"metadata":{"id":"ze8jkchYAM9W","outputId":"8b8b2ddd-84f4-4aba-c0f5-c1b9989c9870","executionInfo":{"status":"ok","timestamp":1641382092609,"user_tz":-540,"elapsed":245,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# 学習済みモデルの読み込み\n","ckpt = torch.load(ckpt_path) # cpuで処理する場合はmap_locationで指定する必要があります。\n","model.load_state_dict(ckpt)\n","model.eval()"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3725, 256, padding_idx=0)\n","    (gru): GRU(256, 256)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(4405, 256, padding_idx=0)\n","    (gru): GRU(256, 256)\n","    (out): Linear(in_features=256, out_features=4405, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":33}]},{"metadata":{"id":"YyKW9WY6AM9Y","executionInfo":{"status":"ok","timestamp":1641382094710,"user_tz":-540,"elapsed":214,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["def ids_to_sentence(vocab, ids):\n","    # IDのリストを単語のリストに変換する\n","    return [vocab.id2word[_id] for _id in ids]\n","\n","def trim_eos(ids):\n","    # IDのリストからEOS以降の単語を除外する\n","    if EOS in ids:\n","        return ids[:ids.index(EOS)]\n","    else:\n","        return ids"],"execution_count":34,"outputs":[]},{"metadata":{"id":"r7qCpnSpAM9b","executionInfo":{"status":"ok","timestamp":1641382096541,"user_tz":-540,"elapsed":232,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["# テストデータの読み込み\n","test_X = load_data('./data/dev.en')\n","test_Y = load_data('./data/dev.ja')"],"execution_count":35,"outputs":[]},{"metadata":{"id":"41hLJdkNAM9d","executionInfo":{"status":"ok","timestamp":1641382098107,"user_tz":-540,"elapsed":342,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["test_X = [sentence_to_ids(vocab_X, sentence) for sentence in test_X]\n","test_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in test_Y]"],"execution_count":36,"outputs":[]},{"metadata":{"id":"IFK0JzSYAM9m","executionInfo":{"status":"ok","timestamp":1641382099250,"user_tz":-540,"elapsed":219,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["test_dataloader = DataLoader(test_X, test_Y, batch_size=1, shuffle=False)"],"execution_count":37,"outputs":[]},{"metadata":{"id":"mYfjq3shAM9q","outputId":"bfae6c68-93d6-49fe-dd86-d58c2e08f0b7","executionInfo":{"status":"ok","timestamp":1641382100450,"user_tz":-540,"elapsed":2,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# 生成\n","batch_X, batch_Y, lengths_X = next(test_dataloader)\n","sentence_X = ' '.join(ids_to_sentence(vocab_X, batch_X.data.cpu().numpy()[:-1, 0]))\n","sentence_Y = ' '.join(ids_to_sentence(vocab_Y, batch_Y.data.cpu().numpy()[:-1, 0]))\n","print('src: {}'.format(sentence_X))\n","print('tgt: {}'.format(sentence_Y))\n","\n","output = model(batch_X, lengths_X, max_length=20)\n","output = output.max(dim=-1)[1].view(-1).data.cpu().tolist()\n","output_sentence = ' '.join(ids_to_sentence(vocab_Y, trim_eos(output)))\n","output_sentence_without_trim = ' '.join(ids_to_sentence(vocab_Y, output))\n","print('out: {}'.format(output_sentence))\n","print('without trim: {}'.format(output_sentence_without_trim))"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["src: show your own business .\n","tgt: 自分 の 事 を しろ 。\n","out: 自分 の 仕事 よけい し なさ い 。\n","without trim: 自分 の 仕事 よけい し なさ い 。 </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n"]}]},{"metadata":{"id":"atSEiLMHAM93","outputId":"c1631c84-645c-43a6-b19e-346238327176","executionInfo":{"status":"ok","timestamp":1641382108795,"user_tz":-540,"elapsed":5218,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# BLEUの計算\n","test_dataloader = DataLoader(test_X, test_Y, batch_size=1, shuffle=False)\n","refs_list = []\n","hyp_list = []\n","\n","for batch in test_dataloader:\n","    batch_X, batch_Y, lengths_X = batch\n","    pred_Y = model(batch_X, lengths_X, max_length=20)\n","    pred = pred_Y.max(dim=-1)[1].view(-1).data.cpu().tolist()\n","    refs = batch_Y.view(-1).data.cpu().tolist()\n","    refs_list.append(refs)\n","    hyp_list.append(pred)\n","bleu = calc_bleu(refs_list, hyp_list)\n","print(bleu)"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["16.84790430668735\n"]}]},{"metadata":{"id":"B1UDdsNvruEi"},"cell_type":"markdown","source":["### Beam Search\n","テストデータに対して新たな文を生成する際、これまでは各時刻で最も確率の高い単語を正解として採用し、次のステップでの入力として使っていました。\n","ただ、本当にやりたいのは、文全体の尤度が最も高くなるような文を生成することです。そのため、ただ近視眼的に確率の高い単語を採用していくより、もう少し大局的に評価していく必要があります。\n","\n","Beam Searchでは、各時刻において一定の数$K$のそれまでのスコア(対数尤度など)の高い文を保持しながら選択を行っていきます。  \n","\n","\n","図はSlack上のものを参照してください。"]},{"metadata":{"id":"2vFRiqFwtKsZ","executionInfo":{"status":"ok","timestamp":1641382853604,"user_tz":-540,"elapsed":214,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["class BeamEncoderDecoder(EncoderDecoder):\n","    \"\"\"\n","    Beam Searchでdecodeを行うためのクラス\n","    \"\"\"\n","    def __init__(self, input_size, output_size, hidden_size, beam_size=4):\n","        \"\"\"\n","        :param input_size: int, 入力言語の語彙数\n","        :param output_size: int, 出力言語の語彙数\n","        :param hidden_size: int, 隠れ層のユニット数\n","        :param beam_size: int, ビーム数\n","        \"\"\"\n","        super(BeamEncoderDecoder, self).__init__(input_size, output_size, hidden_size)\n","        self.beam_size = beam_size\n","\n","    def forward(self, batch_X, lengths_X, max_length):\n","        \"\"\"\n","        :param batch_X: tensor, 入力系列のバッチ, size=(max_length, batch_size)\n","        :param lengths_X: list, 入力系列のバッチ内の各サンプルの文長\n","        :param max_length: int, Decoderの最大文長\n","        :return decoder_outputs: list, 各ビームのDecoderの出力\n","        :return finished_scores: list of float, 各ビームのスコア\n","        \"\"\"\n","        _, encoder_hidden = self.encoder(batch_X, lengths_X)\n","\n","        # decoderの入力と隠れ層の初期状態を定義\n","        decoder_input = torch.tensor([BOS] * self.beam_size, dtype=torch.long, device=device)\n","        decoder_input = decoder_input.unsqueeze(0)  # (1, batch_size)\n","        decoder_hidden = encoder_hidden\n","\n","        # beam_sizeの数だけrepeatする\n","        decoder_input = decoder_input.expand(1, beam_size)\n","        decoder_hidden = decoder_hidden.expand(1, beam_size, -1).contiguous()\n","\n","        k = beam_size\n","        finished_beams = []\n","        finished_scores = []\n","        prev_probs = torch.zeros(beam_size, 1, dtype=torch.float, device=device)  # 前の時刻の各ビームの対数尤度を保持しておく\n","        output_size = self.decoder.output_size\n","\n","        # 各時刻ごとに処理\n","        for t in range(max_length):\n","            # decoder_input: (1, k)\n","            decoder_output, decoder_hidden = self.decoder(decoder_input[-1:], decoder_hidden)\n","            # decoder_output: (1, k, output_size)\n","            # decoder_hidden: (1, k, hidden_size)\n","            decoder_output_t = decoder_output[-1]  # (k, output_size)\n","            log_probs = prev_probs + F.log_softmax(decoder_output_t, dim=-1)  # (k, output_size)\n","            scores = log_probs  # 対数尤度をスコアとする\n","\n","            # スコアの高いビームとその単語を取得\n","            flat_scores = scores.view(-1)  # (k*output_size,)\n","            if t == 0:\n","                flat_scores = flat_scores[:output_size]  # t=0のときは後半の同じ値の繰り返しを除外\n","            top_vs, top_is = flat_scores.data.topk(k)\n","            beam_indices = top_is / output_size  # (k,)\n","            word_indices = top_is % output_size  # (k,)\n","            \n","            # ビームを更新する\n","            _next_beam_indices = []\n","            _next_word_indices = []\n","            for b, w in zip(beam_indices, word_indices):\n","                if w.item() == EOS:  # EOSに到達した場合はそのビームは更新して終了\n","                    k -= 1\n","                    beam = torch.cat([decoder_input.t()[b], w.view(1,)])  # (t+2,)\n","                    score = scores[b, w].item()\n","                    finished_beams.append(beam)\n","                    finished_scores.append(score)\n","                else:   # それ以外の場合はビームを更新\n","                    _next_beam_indices.append(b)\n","                    _next_word_indices.append(w)\n","            if k == 0:\n","                break\n","\n","            # tensorｎに変換\n","            next_beam_indices = torch.tensor(_next_beam_indices, device=device)\n","            next_word_indices = torch.tensor(_next_word_indices, device=device)\n","\n","            # 次の時刻のDecoderの入力を更新\n","            decoder_input = torch.index_select(\n","                decoder_input, dim=-1, index=next_beam_indices)\n","            decoder_input = torch.cat(\n","                [decoder_input, next_word_indices.unsqueeze(0)], dim=0)\n","    \n","            # 次の時刻のDecoderの隠れ層を更新\n","            decoder_hidden = torch.index_select(\n","                decoder_hidden, dim=1, index=next_beam_indices)\n","\n","            # 各ビームの対数尤度を更新\n","            flat_probs = log_probs.view(-1)  # (k*output_size,)\n","            next_indices = (next_beam_indices + 1) * next_word_indices\n","            prev_probs = torch.index_select(\n","                flat_probs, dim=0, index=next_indices).unsqueeze(1)  # (k, 1)\n","\n","        # すべてのビームが完了したらデータを整形\n","        decoder_outputs = [[idx.item() for idx in beam[1:-1]] for beam in finished_beams]\n","        \n","        return decoder_outputs, finished_scores"],"execution_count":44,"outputs":[]},{"metadata":{"id":"DIZ9NHXHttCg","outputId":"41b1e776-c5cb-455a-e7d2-b3121afffef9","executionInfo":{"status":"ok","timestamp":1641382859163,"user_tz":-540,"elapsed":202,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# 学習済みモデルの読み込み\n","beam_size = 3\n","beam_model = BeamEncoderDecoder(**model_args, beam_size=beam_size).to(device)\n","beam_model.load_state_dict(ckpt)\n","beam_model.eval()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BeamEncoderDecoder(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3725, 256, padding_idx=0)\n","    (gru): GRU(256, 256)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(4405, 256, padding_idx=0)\n","    (gru): GRU(256, 256)\n","    (out): Linear(in_features=256, out_features=4405, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":45}]},{"metadata":{"id":"KzsI_6bBtwGc","executionInfo":{"status":"ok","timestamp":1641382863871,"user_tz":-540,"elapsed":218,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}}},"cell_type":"code","source":["test_dataloader = DataLoader(test_X, test_Y, batch_size=1, shuffle=False)"],"execution_count":46,"outputs":[]},{"metadata":{"id":"iSr_wwtbtxKZ","outputId":"2be6030f-b0fb-4c1a-f533-5501e06d3a82","executionInfo":{"status":"error","timestamp":1641382865592,"user_tz":-540,"elapsed":359,"user":{"displayName":"富樫正尚","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11564743336490416698"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"cell_type":"code","source":["# 生成\n","batch_X, batch_Y, lengths_X = next(test_dataloader)\n","sentence_X = ' '.join(ids_to_sentence(vocab_X, batch_X.data.cpu().numpy()[:-1, 0]))\n","sentence_Y = ' '.join(ids_to_sentence(vocab_Y, batch_Y.data.cpu().numpy()[:-1, 0]))\n","print('src: {}'.format(sentence_X))\n","print('tgt: {}'.format(sentence_Y))\n","\n","# 普通のdecode\n","output = model(batch_X, lengths_X, max_length=20)\n","output = output.max(dim=-1)[1].view(-1).data.cpu().tolist()\n","output_sentence = ' '.join(ids_to_sentence(vocab_Y, trim_eos(output)))\n","print('out: {}'.format(output_sentence))\n","\n","# beam decode\n","outputs, scores = beam_model(batch_X, lengths_X, max_length=20)\n","# scoreの良い順にソート\n","outputs, scores = zip(*sorted(zip(outputs, scores), key=lambda x: -x[1]))\n","for o, output in enumerate(outputs):\n","    output_sentence = ' '.join(ids_to_sentence(vocab_Y, output))\n","    print('out{}: {}'.format(o+1, output_sentence))    "],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["src: show your own business .\n","tgt: 自分 の 事 を しろ 。\n","out: 自分 の 仕事 よけい し なさ い 。\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-3a5868882ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# beam decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# scoreの良い順にソート\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-e193eb075a34>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_X, lengths_X, max_length)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# 次の時刻のDecoderの入力を更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             decoder_input = torch.index_select(\n\u001b[0;32m---> 80\u001b[0;31m                 decoder_input, dim=-1, index=next_beam_indices)\n\u001b[0m\u001b[1;32m     81\u001b[0m             decoder_input = torch.cat(\n\u001b[1;32m     82\u001b[0m                 [decoder_input, next_word_indices.unsqueeze(0)], dim=0)\n","\u001b[0;31mRuntimeError\u001b[0m: \"index_select_out_cuda_impl\" not implemented for 'Float'"]}]},{"metadata":{"id":"roOpX8nAAM95"},"cell_type":"markdown","source":["# 参考文献\n","- [Practical PyTorch: Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb)\n","- [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py)\n","- [Encoder\\-decoderモデルとTeacher Forcing，Scheduled Sampling，Professor Forcing](http://satopirka.com/2018/02/encoder-decoder%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8teacher-forcingscheduled-samplingprofessor-forcing/)\n","- [Sequence\\-to\\-Sequence Learning as Beam\\-Search Optimization](https://arxiv.org/abs/1606.02960)"]}]}